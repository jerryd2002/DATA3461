{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf8381",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "Scikit learn provides a large variety of algorithms for some common Machine Learning tasks, such as:\n",
    "\n",
    "* Classification\n",
    "* Regression\n",
    "* Clustering\n",
    "* Feature Selection\n",
    "* Anomaly Detection\n",
    "\n",
    "It also provides some datasets that you can use to test these algorithms:\n",
    "\n",
    "* Classification Datasets:\n",
    "    * Breast cancer wisconsin\n",
    "    * Iris plants (3-classes)\n",
    "    * Optical recognition of handwritten digits (10-classes)\n",
    "    * Wine (n-classes)\n",
    "\n",
    "* Regression Datasets: \n",
    "    * Boston house prices \n",
    "    * Diabetes\n",
    "    * Linnerrud (multiple regression)\n",
    "    * California Housing\n",
    "\n",
    "* Image: \n",
    "    * The Olivetti faces\n",
    "    * The Labeled Faces in the Wild face recognition\n",
    "    * Forest covertypes\n",
    "\n",
    "* NLP:\n",
    "    * News group\n",
    "    * Reuters Corpus Volume I \n",
    "\n",
    "* Other:\n",
    "    * Kddcup 99- Intrusion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3b6e9",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Use the full [Kddcup](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset to compare classification performance of 3 different classifiers. \n",
    "    * Separate the data into train, validation, and test. \n",
    "    * Use accuracy as the metric for assessing performance. \n",
    "    * For each classifier, identify the hyperparameters. Perform optimization over at least 2 hyperparameters.   \n",
    "    * Compare the performance of the optimal configuration of the classifiers.\n",
    "\n",
    "2. Pick the best algorithm in question 1. Create an ensemble of at least 25 models, and use them for the classification task. Identify the top and bottom 10% of the data in terms of uncertainty of the decision.\n",
    "\n",
    "3. Use 2 different feature selection algorithm to identify the 10 most important features for the task in question 1. Retrain classifiers in question 1 with just this subset of features and compare performance.\n",
    "\n",
    "4. Use the same data, removing the labels, and compare performance of 3 different clustering algorithms. Can you find clusters for each of the classes in question 1? \n",
    "\n",
    "5. Can you identify any clusters within the top/botton 10% identified in 2. What are their characteristics?\n",
    "\n",
    "6. Use the \"SA\" dataset to compare the performance of 3 different anomaly detection algorithms.\n",
    "\n",
    "7. Create a subsample of 250 datapoints, redo question 6, using Leave-one-out as the method of evaluation.\n",
    "\n",
    "8. Use the feature selection algorithm to identify the 5 most important features for the task in question 6, for each algorithm. Does the anomaly detection improve using less features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18662c",
   "metadata": {},
   "source": [
    "## Quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1c631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "D=fetch_kddcup99()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d561eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875d2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _kddcup99_dataset:\n",
      "\n",
      "Kddcup 99 dataset\n",
      "-----------------\n",
      "\n",
      "The KDD Cup '99 dataset was created by processing the tcpdump portions\n",
      "of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n",
      "created by MIT Lincoln Lab [2]_. The artificial data (described on the `dataset's\n",
      "homepage <https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was\n",
      "generated using a closed network and hand-injected attacks to produce a\n",
      "large number of different types of attack with normal activity in the\n",
      "background. As the initial goal was to produce a large training set for\n",
      "supervised learning algorithms, there is a large proportion (80.1%) of\n",
      "abnormal data which is unrealistic in real world, and inappropriate for\n",
      "unsupervised anomaly detection which aims at detecting 'abnormal' data, i.e.:\n",
      "\n",
      "* qualitatively different from normal data\n",
      "* in large minority among the observations.\n",
      "\n",
      "We thus transform the KDD Data set into two different data sets: SA and SF.\n",
      "\n",
      "* SA is obtained by simply selecting all the normal data, and a small\n",
      "  proportion of abnormal data to gives an anomaly proportion of 1%.\n",
      "\n",
      "* SF is obtained as in [3]_\n",
      "  by simply picking up the data whose attribute logged_in is positive, thus\n",
      "  focusing on the intrusion attack, which gives a proportion of 0.3% of\n",
      "  attack.\n",
      "\n",
      "* http and smtp are two subsets of SF corresponding with third feature\n",
      "  equal to 'http' (resp. to 'smtp').\n",
      "\n",
      "General KDD structure :\n",
      "\n",
      "    ================      ==========================================\n",
      "    Samples total         4898431\n",
      "    Dimensionality        41\n",
      "    Features              discrete (int) or continuous (float)\n",
      "    Targets               str, 'normal.' or name of the anomaly type\n",
      "    ================      ==========================================\n",
      "\n",
      "    SA structure :\n",
      "\n",
      "    ================      ==========================================\n",
      "    Samples total         976158\n",
      "    Dimensionality        41\n",
      "    Features              discrete (int) or continuous (float)\n",
      "    Targets               str, 'normal.' or name of the anomaly type\n",
      "    ================      ==========================================\n",
      "\n",
      "    SF structure :\n",
      "\n",
      "    ================      ==========================================\n",
      "    Samples total         699691\n",
      "    Dimensionality        4\n",
      "    Features              discrete (int) or continuous (float)\n",
      "    Targets               str, 'normal.' or name of the anomaly type\n",
      "    ================      ==========================================\n",
      "\n",
      "    http structure :\n",
      "\n",
      "    ================      ==========================================\n",
      "    Samples total         619052\n",
      "    Dimensionality        3\n",
      "    Features              discrete (int) or continuous (float)\n",
      "    Targets               str, 'normal.' or name of the anomaly type\n",
      "    ================      ==========================================\n",
      "\n",
      "    smtp structure :\n",
      "\n",
      "    ================      ==========================================\n",
      "    Samples total         95373\n",
      "    Dimensionality        3\n",
      "    Features              discrete (int) or continuous (float)\n",
      "    Targets               str, 'normal.' or name of the anomaly type\n",
      "    ================      ==========================================\n",
      "\n",
      ":func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it\n",
      "returns a dictionary-like object with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``. The \"as_frame\" optional argument converts\n",
      "``data`` into a pandas DataFrame and ``target`` into a pandas Series. The\n",
      "dataset will be downloaded from the web if necessary.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    .. [2] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n",
      "           Detection Evaluation, Richard Lippmann, Joshua W. Haines,\n",
      "           David J. Fried, Jonathan Korba, Kumar Das.\n",
      "\n",
      "    .. [3] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\n",
      "           unsupervised outlier detection using finite mixtures with\n",
      "           discounting learning algorithms. In Proceedings of the sixth\n",
      "           ACM SIGKDD international conference on Knowledge discovery\n",
      "           and data mining, pages 320-324. ACM Press, 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(D[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c3c5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cef559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'back.', b'buffer_overflow.', b'ftp_write.', b'guess_passwd.',\n",
       "       b'imap.', b'ipsweep.', b'land.', b'loadmodule.', b'multihop.',\n",
       "       b'neptune.', b'nmap.', b'normal.', b'perl.', b'phf.', b'pod.',\n",
       "       b'portsweep.', b'rootkit.', b'satan.', b'smurf.', b'spy.',\n",
       "       b'teardrop.', b'warezclient.', b'warezmaster.'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(D[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed0289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(D[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff034ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration',\n",
       " 'protocol_type',\n",
       " 'service',\n",
       " 'flag',\n",
       " 'src_bytes',\n",
       " 'dst_bytes',\n",
       " 'land',\n",
       " 'wrong_fragment',\n",
       " 'urgent',\n",
       " 'hot',\n",
       " 'num_failed_logins',\n",
       " 'logged_in',\n",
       " 'num_compromised',\n",
       " 'root_shell',\n",
       " 'su_attempted',\n",
       " 'num_root',\n",
       " 'num_file_creations',\n",
       " 'num_shells',\n",
       " 'num_access_files',\n",
       " 'num_outbound_cmds',\n",
       " 'is_host_login',\n",
       " 'is_guest_login',\n",
       " 'count',\n",
       " 'srv_count',\n",
       " 'serror_rate',\n",
       " 'srv_serror_rate',\n",
       " 'rerror_rate',\n",
       " 'srv_rerror_rate',\n",
       " 'same_srv_rate',\n",
       " 'diff_srv_rate',\n",
       " 'srv_diff_host_rate',\n",
       " 'dst_host_count',\n",
       " 'dst_host_srv_count',\n",
       " 'dst_host_same_srv_rate',\n",
       " 'dst_host_diff_srv_rate',\n",
       " 'dst_host_same_src_port_rate',\n",
       " 'dst_host_srv_diff_host_rate',\n",
       " 'dst_host_serror_rate',\n",
       " 'dst_host_srv_serror_rate',\n",
       " 'dst_host_rerror_rate',\n",
       " 'dst_host_srv_rerror_rate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[\"feature_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e63c7",
   "metadata": {},
   "source": [
    "# Excerise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087676b9",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52280615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e38bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(D['data'], columns=D['feature_names'])\n",
    "\n",
    "#Assign features and target variables\n",
    "X = df \n",
    "y = D['target'] \n",
    "\n",
    "y = (y == b'normal.').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ebba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing transformations\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7bbfd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e7b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Logistic Regression Validation Accuracy: 0.9997065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train a Base Logistic Regression Model\n",
    "base_lr = LogisticRegression(solver='liblinear')\n",
    "base_lr.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate Base Logistic Regression on Validation Set\n",
    "y_val_pred = base_lr.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Base Logistic Regression Validation Accuracy: {val_acc:.7f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5038da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
      "Logistic Regression Best Validation Accuracy: 0.9997\n",
      "\n",
      "Logistic Regression Test Accuracy: 0.9997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train Logistic Regression with Hyperparameter Tuning\n",
    "param_grid_lr = {'C': [0.1, 1, 10], 'penalty': ['l2']}\n",
    "logistic_regression = LogisticRegression(solver='liblinear')\n",
    "grid_search_lr = GridSearchCV(logistic_regression, param_grid_lr, cv=5, n_jobs=-1)\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "#Best parameters and validation score\n",
    "print(f\"Logistic Regression Best Parameters: {grid_search_lr.best_params_}\")\n",
    "print(f\"Logistic Regression Best Validation Accuracy: {grid_search_lr.best_score_:.4f}\\n\")\n",
    "\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_pred = best_lr.predict(X_test)\n",
    "acc_lr = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Test Accuracy: {acc_lr:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5067fb",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base SVM Model\n",
    "base_svm = SVC()\n",
    "base_svm.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate Base SVM on Validation Set\n",
    "y_val_pred_svm = base_svm.predict(X_val)\n",
    "val_acc_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "print(f\"Base SVM Validation Accuracy: {val_acc_svm:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90198a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVM with Hyperparameter Tuning\n",
    "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "svm = SVC()\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, n_jobs=-1)\n",
    "grid_search_svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters and validation score for SVM\n",
    "print(f\"SVM Best Parameters: {grid_search_svm.best_params_}\")\n",
    "print(f\"SVM Best Validation Accuracy: {grid_search_svm.best_score_:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dab273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate SVM on Test Set\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Test Accuracy: {acc_svm:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6f349",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198390aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base Decision Tree Model\n",
    "base_dt = DecisionTreeClassifier()\n",
    "base_dt.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate Base Decision Tree on Validation Set\n",
    "y_val_pred_dt = base_dt.predict(X_val)\n",
    "val_acc_dt = accuracy_score(y_val, y_val_pred_dt)\n",
    "print(f\"Base Decision Tree Validation Accuracy: {val_acc_dt:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Decision Tree with Hyperparameter Tuning\n",
    "param_grid_dt = {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10, 20]}\n",
    "dt = DecisionTreeClassifier()\n",
    "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1)\n",
    "grid_search_dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters and validation score for Decision Tree\n",
    "print(f\"Decision Tree Best Parameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Decision Tree Best Validation Accuracy: {grid_search_dt.best_score_:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree on Test Set\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Test Accuracy: {acc_dt:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be80d9a",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f056e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "#25 model ensemble \n",
    "n_models = 25\n",
    "models = []\n",
    "\n",
    "#Train 25 Logistic Regression models on subsets\n",
    "for i in range(n_models):\n",
    "    X_train_sample, y_train_sample = resample(X_train, y_train, random_state=i)\n",
    "    lr = LogisticRegression(max_iter=1000, solver='liblinear', C=10, penalty='l2')\n",
    "    lr.fit(X_train_sample, y_train_sample)\n",
    "    models.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2e4080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10% most uncertain samples:\n",
      "  (0, 0)\t1.0\n",
      "  (0, 2495)\t1.0\n",
      "  (0, 2512)\t1.0\n",
      "  (0, 2573)\t1.0\n",
      "  (0, 3066)\t1.0\n",
      "  (0, 5875)\t1.0\n",
      "  (0, 16600)\t1.0\n",
      "  (0, 16602)\t1.0\n",
      "  (0, 16605)\t1.0\n",
      "  (0, 16609)\t1.0\n",
      "  (0, 16631)\t1.0\n",
      "  (0, 16637)\t1.0\n",
      "  (0, 16639)\t1.0\n",
      "  (0, 16662)\t1.0\n",
      "  (0, 16664)\t1.0\n",
      "  (0, 16667)\t1.0\n",
      "  (0, 16687)\t1.0\n",
      "  (0, 16705)\t1.0\n",
      "  (0, 16708)\t1.0\n",
      "  (0, 16715)\t1.0\n",
      "  (0, 16716)\t1.0\n",
      "  (0, 16717)\t1.0\n",
      "  (0, 17174)\t1.0\n",
      "  (0, 17644)\t1.0\n",
      "  (0, 17679)\t1.0\n",
      "  :\t:\n",
      "  (9879, 16687)\t1.0\n",
      "  (9879, 16705)\t1.0\n",
      "  (9879, 16708)\t1.0\n",
      "  (9879, 16715)\t1.0\n",
      "  (9879, 16716)\t1.0\n",
      "  (9879, 16717)\t1.0\n",
      "  (9879, 16723)\t1.0\n",
      "  (9879, 17213)\t1.0\n",
      "  (9879, 17679)\t1.0\n",
      "  (9879, 17771)\t1.0\n",
      "  (9879, 17822)\t1.0\n",
      "  (9879, 17899)\t1.0\n",
      "  (9879, 18024)\t1.0\n",
      "  (9879, 18096)\t1.0\n",
      "  (9879, 18173)\t1.0\n",
      "  (9879, 18446)\t1.0\n",
      "  (9879, 18450)\t1.0\n",
      "  (9879, 18704)\t1.0\n",
      "  (9879, 18835)\t1.0\n",
      "  (9879, 18905)\t1.0\n",
      "  (9879, 19006)\t1.0\n",
      "  (9879, 19071)\t1.0\n",
      "  (9879, 19171)\t1.0\n",
      "  (9879, 19256)\t1.0\n",
      "  (9879, 19344)\t1.0\n",
      "\n",
      "Bottom 10% least uncertain samples:\n",
      "  (0, 0)\t1.0\n",
      "  (0, 2496)\t1.0\n",
      "  (0, 2543)\t1.0\n",
      "  (0, 2569)\t1.0\n",
      "  (0, 2575)\t1.0\n",
      "  (0, 5875)\t1.0\n",
      "  (0, 16600)\t1.0\n",
      "  (0, 16602)\t1.0\n",
      "  (0, 16605)\t1.0\n",
      "  (0, 16609)\t1.0\n",
      "  (0, 16631)\t1.0\n",
      "  (0, 16637)\t1.0\n",
      "  (0, 16639)\t1.0\n",
      "  (0, 16662)\t1.0\n",
      "  (0, 16664)\t1.0\n",
      "  (0, 16667)\t1.0\n",
      "  (0, 16687)\t1.0\n",
      "  (0, 16705)\t1.0\n",
      "  (0, 16708)\t1.0\n",
      "  (0, 16715)\t1.0\n",
      "  (0, 16716)\t1.0\n",
      "  (0, 16717)\t1.0\n",
      "  (0, 16997)\t1.0\n",
      "  (0, 17223)\t1.0\n",
      "  (0, 17770)\t1.0\n",
      "  :\t:\n",
      "  (9879, 16687)\t1.0\n",
      "  (9879, 16705)\t1.0\n",
      "  (9879, 16708)\t1.0\n",
      "  (9879, 16715)\t1.0\n",
      "  (9879, 16716)\t1.0\n",
      "  (9879, 16717)\t1.0\n",
      "  (9879, 16830)\t1.0\n",
      "  (9879, 17217)\t1.0\n",
      "  (9879, 17770)\t1.0\n",
      "  (9879, 17821)\t1.0\n",
      "  (9879, 17822)\t1.0\n",
      "  (9879, 17899)\t1.0\n",
      "  (9879, 17957)\t1.0\n",
      "  (9879, 18055)\t1.0\n",
      "  (9879, 18127)\t1.0\n",
      "  (9879, 18446)\t1.0\n",
      "  (9879, 18455)\t1.0\n",
      "  (9879, 18706)\t1.0\n",
      "  (9879, 18809)\t1.0\n",
      "  (9879, 18905)\t1.0\n",
      "  (9879, 19006)\t1.0\n",
      "  (9879, 19170)\t1.0\n",
      "  (9879, 19242)\t1.0\n",
      "  (9879, 19243)\t1.0\n",
      "  (9879, 19344)\t1.0\n"
     ]
    }
   ],
   "source": [
    "#Initializing matrix to store the probabilities for the models\n",
    "ensemble_probs = np.zeros((X_test.shape[0], n_models))\n",
    "\n",
    "#Probability prediction for each model in the ensemble\n",
    "for i, model in enumerate(models):\n",
    "    ensemble_probs[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#Average prediction\n",
    "average_probs = np.mean(ensemble_probs, axis=1)\n",
    "\n",
    "#Variance Calculations\n",
    "uncertainty = np.var(ensemble_probs, axis=1)\n",
    "\n",
    "#Sorting of top and bottom\n",
    "n_top_bottom = int(0.1 * X_test.shape[0])\n",
    "sorted_indices = np.argsort(uncertainty)\n",
    "\n",
    "#The top 10% of the data in terms of uncertainty\n",
    "top_10_percent_indices = sorted_indices[-n_top_bottom:]\n",
    "\n",
    "#The bottom 10% of the data in terms of uncertainty\n",
    "bottom_10_percent_indices = sorted_indices[:n_top_bottom]\n",
    "\n",
    "print(\"Top 10% most uncertain samples:\")\n",
    "print(X_test[top_10_percent_indices])\n",
    "print(\"\\nBottom 10% least uncertain samples:\")\n",
    "print(X_test[bottom_10_percent_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe6391",
   "metadata": {},
   "source": [
    "# Exercise 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Feature Selection using RFE\n",
    "rfe = RFE(estimator=best_lr, n_features_to_select=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "#Train Logistic Regression using RFE-selected features\n",
    "lr_rfe = LogisticRegression(max_iter=1000, solver='liblinear', C=10, penalty='l2')\n",
    "lr_rfe.fit(X_train_rfe, y_train)\n",
    "y_pred_rfe = lr_rfe.predict(X_test_rfe)\n",
    "acc_rfe = accuracy_score(y_test, y_pred_rfe)\n",
    "print(f\"Logistic Regression Test Accuracy with RFE-selected features: {acc_rfe:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection using Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features_indices = np.argsort(feature_importances)[-10:]\n",
    "\n",
    "#Select the top 10 important features for train and test sets\n",
    "X_train_rf = X_train[:, important_features_indices]\n",
    "X_test_rf = X_test[:, important_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Logistic Regression using Random Forest-selected features\n",
    "lr_rf = LogisticRegression(max_iter=1000, solver='liblinear', C=10, penalty='l2')\n",
    "lr_rf.fit(X_train_rf, y_train)\n",
    "y_pred_rf = lr_rf.predict(X_test_rf)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Logistic Regression Test Accuracy with Random Forest-selected features: {acc_rf:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9734120",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Analysis\n",
    "X_all = np.vstack([X_train, X_test])\n",
    "y_all = np.hstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d95754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_all)\n",
    "kmeans_ari = adjusted_rand_score(y_all, kmeans_labels)\n",
    "kmeans_silhouette = silhouette_score(X_all, kmeans_labels)\n",
    "print(f\"KMeans ARI: {kmeans_ari:.4f}, Silhouette Score: {kmeans_silhouette:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=2)\n",
    "agg_labels = agg_clustering.fit_predict(X_all)\n",
    "agg_ari = adjusted_rand_score(y_all, agg_labels)\n",
    "agg_silhouette = silhouette_score(X_all, agg_labels)\n",
    "print(f\"Agglomerative Clustering ARI: {agg_ari:.4f}, Silhouette Score: {agg_silhouette:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b128db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN Clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_all)\n",
    "valid_labels = dbscan_labels != -1 \n",
    "if np.sum(valid_labels) > 0:\n",
    "    dbscan_ari = adjusted_rand_score(y_all[valid_labels], dbscan_labels[valid_labels])\n",
    "    dbscan_silhouette = silhouette_score(X_all[valid_labels], dbscan_labels[valid_labels])\n",
    "    print(f\"DBSCAN ARI: {dbscan_ari:.4f}, Silhouette Score: {dbscan_silhouette:.4f}\\n\")\n",
    "else:\n",
    "    print(\"DBSCAN did not find enough clusters to evaluate.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76419e0",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Analysis on Top & Bottom 10%\n",
    "def cluster_analysis(samples, title):\n",
    "    print(f\"Clustering on {title} Samples:\\n\")\n",
    "\n",
    "    # KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(samples)\n",
    "    print(f\"KMeans Labels: {np.unique(kmeans_labels)}\\n\")\n",
    "\n",
    "    # Agglomerative Clustering\n",
    "    agg = AgglomerativeClustering(n_clusters=2)\n",
    "    agg_labels = agg.fit_predict(samples)\n",
    "    print(f\"Agglomerative Clustering Labels: {np.unique(agg_labels)}\\n\")\n",
    "\n",
    "    # DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(samples)\n",
    "    valid_labels = dbscan_labels != -1\n",
    "    if np.sum(valid_labels) > 0:\n",
    "        print(f\"DBSCAN Labels: {np.unique(dbscan_labels[valid_labels])}\\n\")\n",
    "    else:\n",
    "        print(\"DBSCAN did not find enough clusters to evaluate.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce22e30",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
